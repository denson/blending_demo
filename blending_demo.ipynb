{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending Demo\n",
    "\n",
    "I started with the code from a Kaggle 3rd place finisher. The 1st and 2nd place finishers were only very slightly better on the holdout set and had much more complex solutions that might not have performed better on a different test set and certainly would not have generalized to most other problem domains.\n",
    "\n",
    "The general concept is that if we build multiple different models trained on different samples of our training data we get multiple predictions that are substantially better than chance and that are uncorrelated with each other.\n",
    "\n",
    "In step 1 we take stratified fold samples of our training data and build multiple models (in this case RDF entropy,RDF-gini ET-entropy,ET-gini and GBT) on each fold. We then use the trained models to predict the training sample **not** in the training part of this fold. It is super important that you do not use a given model to predict training data that was used to train that model on that fold.\n",
    "We also predict all the test data with each model. These predictions are a way of transforming the training data and the test data into a different space with the predicted probabilities as the transformed information. We take a simple  average of the predictions of each type of model (eg RDF-gini) and that becomes the transformed data for the next step. If we have 5 different models as in this case our input data for step 2 will have 5 columns and the same number of rows as the training set and test set respectively.\n",
    "\n",
    "In step 2 we use a train a logistic regresson on the transformed training data and use it to predict the transformed test data. We take the predicted probabilities from the LR as our final answer. \n",
    "\n",
    "This method usually results in an improvement over a single highly tuned model for \"hard\" problems and not \"simple\" problems. By hard I mean that the decision boundary between classes is highly non-linear. Overlapping classes and non-linear relationships between features contribute to making problems hard.\n",
    "\n",
    "For the synthetic dataset in this demo these were the results:\n",
    "\n",
    "**Blended MCC = 0.6411**\n",
    "**Blended logloss = 0.3925**\n",
    "\n",
    "\n",
    "Single RDF MCC = 0.3794\n",
    "Single RDF logloss = 0.6317\n",
    "\n",
    "\n",
    "Single ET MCC = 0.2114\n",
    "Single ET logloss = 0.6679\n",
    "\n",
    "\n",
    "Single GBT MCC = 0.6182\n",
    "Single GBT logloss = 0.4741\n",
    "\n",
    "Notice that the blended model tied the best log loss and had the best MCC. This is typical for a difficult problem.\n",
    "\n",
    "\n",
    "Kaggle competition: Predicting a Biological Response.\n",
    "\n",
    "Blending {RandomForests, ExtraTrees, GradientBoosting} + stretching to\n",
    "[0,1]. The blending scheme is related to the idea Jose H. Solorzano\n",
    "presented here:\n",
    "http://www.kaggle.com/c/bioresponse/forums/t/1889/question-about-the-process-of-ensemble-learning/\n",
    "'''You can try this: In one of the 5 folds, train the models, then use\n",
    "the results of the models as 'variables' in logistic regression over\n",
    "the validation data of that fold'''. Or at least this is the\n",
    "implementation of my understanding of that idea :-)\n",
    "\n",
    "The predictions are saved in test.csv. The code below created my best\n",
    "submission to the competition:\n",
    "- public score (25%): 0.43464\n",
    "- private score (75%): 0.37751\n",
    "- final rank on the private leaderboard: 17th over 711 teams :-)\n",
    "\n",
    "Note: if you increase the number of estimators of the classifiers,\n",
    "e.g. n_estimators=1000, you get a better score/rank on the private\n",
    "test set.\n",
    "\n",
    "Copyright 2012, Emanuele Olivetti.\n",
    "BSD license, 3 clauses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.18.1.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sklearn_version = sklearn.__version__\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "\n",
    "sklearn_version = sklearn_version.split('.')\n",
    "main_sklearn_verison = int(sklearn_version[1])\n",
    "\n",
    "current_scikit_verison_flag = True\n",
    "\n",
    "if main_sklearn_verison < 18:\n",
    "    print('Your version of scikit learn is less than version 18.')\n",
    "    print('Denson will stop supporting versions less than 18 in March 2017.')\n",
    "    current_scikit_verison_flag = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if current_scikit_verison_flag:\n",
    "    from sklearn.model_selection import StratifiedKFold,StratifiedShuffleSplit\n",
    "else:\n",
    "    from sklearn.cross_validation import StratifiedKFold, StratifiedShuffleSplit\n",
    "    \n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "\n",
    "from sklearn.datasets import  make_classification\n",
    "\n",
    "\n",
    "def logloss(attempt, actual, epsilon=1.0e-15):\n",
    "    \"\"\"Logloss\n",
    "    \"\"\"\n",
    "    attempt = np.clip(attempt, epsilon, 1.0-epsilon)\n",
    "    return - np.mean(actual * np.log(attempt) +\n",
    "                     (1.0 - actual) * np.log(1.0 - attempt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peformance measures\n",
    "\n",
    "In this example we are using Matthews correlation coefficient (MCC) and log loss as\n",
    "performance metrics.\n",
    "\n",
    "MCC is related to chi squared and is a very good way to measure classification\n",
    "performance for binary classification problems or a multiclass classification \n",
    "problem that has been converted to a series of binary classification problems. \n",
    "The main reason is that it works well even with highly imbalanced classes. For \n",
    "example if we have a problem where 98% of the rows are class = 0 and 2% are \n",
    "class = 1, MCC will usually give you a better picture of performance than F1-score\n",
    "or precision/recall. The range of MCC is -1 to 1 with -1 being a perfect negative\n",
    "classifier, 1 a perfect classifier and 0 equal to chance.\n",
    "\n",
    "While not exactly true, it is convenient to think of MCC as how much better we\n",
    "did than chance. If you have a very difficult problem and you have an MCC of\n",
    "30% then our performance is about 30% better than if we had guessed using the class\n",
    "distribution.\n",
    "\n",
    "Log Loss is a good way to measure the confidence of your model. In general, even\n",
    "if we have a very accurate model it is bad if it has high confidence when it is \n",
    "wrong. \n",
    "\n",
    "For example, if we have a model with the following performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true\ty_pred\tproba\n",
      "1\t1\t0.9000\n",
      "1\t1\t0.9000\n",
      "1\t1\t0.9000\n",
      "1\t1\t0.9000\n",
      "1\t0\t0.1000\n",
      "0\t0\t0.1000\n",
      "0\t0\t0.1000\n",
      "0\t0\t0.1000\n",
      "0\t0\t0.1000\n",
      "0\t0\t0.1000\n",
      "log loss = 0.3251\n",
      "MCC = 0.8165\n"
     ]
    }
   ],
   "source": [
    "# Model 1\n",
    "    \n",
    "y_true = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "proba = np.array([0.9,0.9,0.9,0.9,0.1,0.1,0.1,0.1,0.1,0.1])\n",
    "y_pred = np.array([1,1,1,1,0,0,0,0,0,0])\n",
    "log_loss = logloss(proba, y_true, epsilon=1.0e-15)\n",
    "MCC = matthews_corrcoef(y_true,y_pred)\n",
    "print('y_true\\ty_pred\\tproba')\n",
    "for idx in range(len(y_true)):\n",
    "     print('%i\\t%i\\t%.4f' % (y_true[idx],y_pred[idx],proba[idx]))\n",
    "print('log loss = %.4f' % log_loss)\n",
    "print('MCC = %.4f' % MCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true\ty_pred\tproba\n",
      "1\t1\t0.8000\n",
      "1\t1\t0.8000\n",
      "1\t1\t0.8000\n",
      "1\t1\t0.8000\n",
      "1\t0\t0.4000\n",
      "0\t0\t0.2000\n",
      "0\t0\t0.2000\n",
      "0\t0\t0.2000\n",
      "0\t0\t0.2000\n",
      "0\t0\t0.2000\n",
      "log loss = 0.2925\n",
      "MCC = 0.8165\n"
     ]
    }
   ],
   "source": [
    "# Model 2\n",
    "    \n",
    "y_true = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "proba = np.array([0.8,0.8,0.8,0.8,0.4,0.2,0.2,0.2,0.2,0.2])\n",
    "y_pred = np.array([1,1,1,1,0,0,0,0,0,0])\n",
    "log_loss = logloss(proba, y_true, epsilon=1.0e-15)\n",
    "MCC = matthews_corrcoef(y_true,y_pred)\n",
    "print('y_true\\ty_pred\\tproba')\n",
    "for idx in range(len(y_true)):\n",
    "     print('%i\\t%i\\t%.4f' % (y_true[idx],y_pred[idx],proba[idx]))\n",
    "print('log loss = %.4f' % log_loss)\n",
    "print('MCC = %.4f' % MCC)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 is better because although it is less confident when it is right it is\n",
    "much less confident when it is wrong.\n",
    "\n",
    "Blended models such as the one in this demo tend to do well with the log loss measure because we are building diverse models that tend to average results away from overconfident predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin the blending demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)  # seed to shuffle the train set\n",
    "\n",
    "\n",
    "# Number of folds between 5-20 is usually best...your milage may vary\n",
    "n_folds = 10\n",
    "verbose = True\n",
    "shuffle = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic dataset\n",
    "\n",
    "If the problem is \"easy\" a single model will probably outperform a blended model. We are using sklearn make_classification with parameters set so that we have a \"hard\" problem.\n",
    "\n",
    "This is a difficult problem because:\n",
    "\n",
    "    1) There are only 10 relevant features and 440 noise features\n",
    "    2) There is a great deal of overlap between classes\n",
    "    3) There are 3 clusters per class\n",
    "    4) The relationships between featues is non-linear\n",
    "    5) We have a fairly small sample size for the complexity of the problem.\n",
    "\n",
    "Noise features will tend to confuse any machine learning model. In this case we have far more noise features that contain signal.\n",
    "\n",
    "The class_sep parameter controls the distance between the center of each cluster. There is gausian noise within each cluster. When the class_sep < 1 there is a good deal of overlap between classes and the blended model will be best. If the class_sep > 1 then a single model will often beat the blended model. If the class_sep > 2 then a simple model like logistic regression will usually be best.\n",
    "\n",
    "Having multiple clusters per class means that there is certainly no linear boundary between classes. Also, this compounds to problem of overlap between classes.\n",
    "\n",
    "The sklearn make_classification is designed to generate non-linear relationships between features based on thealgorithm described in :\n",
    "\n",
    "Guyon, Isabelle. \"Design of experiments of the NIPS 2003 variable selection benchmark.\" \n",
    "\n",
    "Empirically, as the sample size increases the classification performance will also increase but if the blended model wins it will continue to win.\n",
    "\n",
    "Notice that we are not doing any tuning. It might be possible to tune a GBT or other model to outperform the blended model on a particular sample of data. However, that highly tuned model will likely not generalize as well for new data. In a real problem our available sample for training and testing a model is nearly always biased in some unknown way. A blended model is much more robust to this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for blending.\n",
      "('jdx, clf', 0, RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False))\n",
      "('Fold', 0)\n",
      "('Fold', 1)\n",
      "('Fold', 2)\n",
      "('Fold', 3)\n",
      "('Fold', 4)\n",
      "('Fold', 5)\n",
      "('Fold', 6)\n",
      "('Fold', 7)\n",
      "('Fold', 8)\n",
      "('Fold', 9)\n",
      "('jdx, clf', 1, RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False))\n",
      "('Fold', 0)\n",
      "('Fold', 1)\n",
      "('Fold', 2)\n",
      "('Fold', 3)\n",
      "('Fold', 4)\n",
      "('Fold', 5)\n",
      "('Fold', 6)\n",
      "('Fold', 7)\n",
      "('Fold', 8)\n",
      "('Fold', 9)\n",
      "('jdx, clf', 2, ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False))\n",
      "('Fold', 0)\n",
      "('Fold', 1)\n",
      "('Fold', 2)\n",
      "('Fold', 3)\n",
      "('Fold', 4)\n",
      "('Fold', 5)\n",
      "('Fold', 6)\n",
      "('Fold', 7)\n",
      "('Fold', 8)\n",
      "('Fold', 9)\n",
      "('jdx, clf', 3, ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=-1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False))\n",
      "('Fold', 0)\n",
      "('Fold', 1)\n",
      "('Fold', 2)\n",
      "('Fold', 3)\n",
      "('Fold', 4)\n",
      "('Fold', 5)\n",
      "('Fold', 6)\n",
      "('Fold', 7)\n",
      "('Fold', 8)\n",
      "('Fold', 9)\n",
      "('jdx, clf', 4, GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=6,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=50, presort='auto', random_state=None,\n",
      "              subsample=0.5, verbose=0, warm_start=False))\n",
      "('Fold', 0)\n",
      "('Fold', 1)\n",
      "('Fold', 2)\n",
      "('Fold', 3)\n",
      "('Fold', 4)\n",
      "('Fold', 5)\n",
      "('Fold', 6)\n",
      "('Fold', 7)\n",
      "('Fold', 8)\n",
      "('Fold', 9)\n"
     ]
    }
   ],
   "source": [
    "# In a real problem remove the code between these comments and load your data\n",
    "sample_size = 10000\n",
    "n_features = 450\n",
    "n_informative = 10 \n",
    "num_noise_feats = n_features - n_informative\n",
    "class_sep = 0.5\n",
    "mislabel = 0\n",
    "\n",
    "# This will create mislabled/noisy y labels.\n",
    "flip_y = float(mislabel)/100.0\n",
    "# Generate the problem\n",
    "X_gen, y_gen = make_classification(n_samples=sample_size , \n",
    "                           n_features=n_features , \n",
    "                           n_redundant=0, \n",
    "                           n_informative=n_informative,\n",
    "                           random_state=42, \n",
    "                           n_clusters_per_class=3, \n",
    "                           class_sep = class_sep, \n",
    "                           flip_y = flip_y, \n",
    "                           shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use SSS to create training and test sets\n",
    "\n",
    "if current_scikit_verison_flag:\n",
    "    sss = StratifiedShuffleSplit(n_splits=1,\n",
    "                                 test_size=0.2,\n",
    "                                 random_state=42)\n",
    "    \n",
    "    for train_index,test_index in sss.split(X_gen,y_gen):\n",
    "        X_train = X_gen[train_index] \n",
    "        y_train = y_gen[train_index]\n",
    "\n",
    "        X_test = X_gen[test_index]\n",
    "        y_test = y_gen[test_index]\n",
    "\n",
    "else:\n",
    "    sss = StratifiedShuffleSplit(y_gen, 1, test_size=0.2, random_state=42)\n",
    "\n",
    "    for train_index,test_index in sss:\n",
    "        X_train = X_gen[train_index] \n",
    "        y_train = y_gen[train_index]\n",
    "\n",
    "        X_test = X_gen[test_index]\n",
    "        y_test = y_gen[test_index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_holdout_true = y_test[:]\n",
    "\n",
    "X_holdout = X_test[:,:]\n",
    "\n",
    "# In a real problem remove the code between these comments and load your data \n",
    "\n",
    "\n",
    "\n",
    "# We run the classifiers with different parameters to make the predictions\n",
    "# less correlated.\n",
    "clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "print(\"Creating train and test sets for blending.\")\n",
    "\n",
    "# These arrays will hold the blended predictions\n",
    "dataset_blend_train = np.zeros((X_train.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_holdout.shape[0], len(clfs)))\n",
    "\n",
    "\n",
    "# Use SSS to create training and test sets\n",
    "\n",
    "if current_scikit_verison_flag:\n",
    "    # For each fold train on 80% and test on 20%. If you have a bunch of data\n",
    "    # you might want to make it a 50/50 split.\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_folds,\n",
    "                                 test_size = 0.2)\n",
    "\n",
    "\n",
    "    # We will resuse the same train-test splits for each of the models.\n",
    "    splits = list(sss.split(X_train,y_train))\n",
    "\n",
    "else:\n",
    "    sss = StratifiedShuffleSplit(y_train, n_folds, test_size=0.2)\n",
    "    \n",
    "    splits = list(sss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for jdx, clf in enumerate(clfs):\n",
    "    print('jdx, clf', jdx, clf)\n",
    "\n",
    "    # dataset__blend_test_j is for this fold of this model (RDF-gini, fold 1 etc)\n",
    "    dataset_blend_test_j = np.zeros((X_holdout.shape[0], len(splits)))\n",
    "    for idx, (train, test) in enumerate(splits):\n",
    "        print(\"Fold\", idx)\n",
    "\n",
    "        # Split the training data into train-test sets for this fold\n",
    "        X_fold_train = X_train[train]\n",
    "        y_fold_train = y_train[train]\n",
    "        X_fold_test = X_train[test]\n",
    "        y_fold_test = y_train[test]\n",
    "\n",
    "        # Fit this model on this fold of data\n",
    "        clf.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        # Predict this test fold\n",
    "        y_fold_pred = clf.predict_proba(X_fold_test)[:, 1]\n",
    "\n",
    "        '''\n",
    "        This is where things get slightly confusing. We are using part of the\n",
    "        training data to predict the rest of the training data. We store the\n",
    "        predictions as the transformed training data. A given row of the \n",
    "        training data is likely to be predicted more than once and will wind\n",
    "        up only with the last prediction. There is no absolute guarantee that \n",
    "        every row of the training data will be predicted but it is highly\n",
    "        likely in 10 folds.\n",
    "        '''\n",
    "        # Store the predictions as transformed training data\n",
    "        # jdx is the index of this model (RDF-gini)\n",
    "        dataset_blend_train[test, jdx] = y_fold_pred \n",
    "\n",
    "        '''\n",
    "        Now we use this model and use it to predict the holdout test data.\n",
    "        We store the predictions of each fold of this model and take the mean\n",
    "        at the end to create the transformation for this model.\n",
    "        '''\n",
    "        dataset_blend_test_j[:, idx] = clf.predict_proba(X_holdout)[:, 1]\n",
    "\n",
    "    # Take the mean prediction of each fold and use it as the transformed\n",
    "    # test data.\n",
    "    dataset_blend_test[:, jdx] = dataset_blend_test_j.mean(1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Blending.\n",
      "Linear stretch of predictions to [0,1]\n",
      "\n",
      "\n",
      "Blended MCC = 0.6411\n",
      "Blended logloss = 0.3925\n",
      "\n",
      "\n",
      "Single RDF MCC = 0.3794\n",
      "Single RDF logloss = 0.6317\n",
      "\n",
      "\n",
      "Single ET MCC = 0.2114\n",
      "Single ET logloss = 0.6679\n",
      "\n",
      "\n",
      "Single GBT MCC = 0.6182\n",
      "Single GBT logloss = 0.4741\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now we train a logistic regression (or some other simple model) on the transformed\n",
    "training data and use it to predict the transformed test data.\n",
    "'''\n",
    "print\n",
    "print(\"Blending.\")\n",
    "clf = LogisticRegression()\n",
    "clf.fit(dataset_blend_train, y_train)\n",
    "y_holdout = clf.predict_proba(dataset_blend_test)[:, 1]\n",
    "\n",
    "'''\n",
    "It is possible that the predictions from the logistic regression on the \n",
    "transformed data will be skewed towards 0 or 1. This will stretch the predictions\n",
    "back to a range of 0-1\n",
    "'''\n",
    "print(\"Linear stretch of predictions to [0,1]\")\n",
    "y_holdout = (y_holdout - y_holdout.min()) / (y_holdout.max() - y_holdout.min())\n",
    "\n",
    "y_pred = np.zeros(len(y_holdout))\n",
    "\n",
    "# Convert the probabilities to  integer predictions of class 0 or class 1\n",
    "class_one_rows = np.where(y_holdout >= 0.5)[0]\n",
    "\n",
    "y_pred[class_one_rows] = 1\n",
    "\n",
    "# Compute some performance metrics\n",
    "print('')\n",
    "print('')\n",
    "MCC = matthews_corrcoef(y_holdout_true,y_pred)\n",
    "print('Blended MCC = %.4f' % MCC)\n",
    "\n",
    "log_loss = logloss(y_holdout, y_holdout_true, epsilon=1.0e-15)\n",
    "print('Blended logloss = %.4f' % log_loss)    \n",
    "\n",
    "\n",
    "# Train single models for comparison\n",
    "clf = RandomForestClassifier(n_estimators=100, \n",
    "                             n_jobs=-1, \n",
    "                             criterion='entropy')\n",
    "clf.fit(X_train,y_train)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred = np.zeros(len(y_holdout_true))\n",
    "\n",
    "class_1_rows = np.where(y_proba >= 0.5)[0]\n",
    "\n",
    "y_pred[class_1_rows] = 1\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "MCC = matthews_corrcoef(y_holdout_true,y_pred)\n",
    "print('Single RDF MCC = %.4f' % MCC)\n",
    "\n",
    "log_loss = logloss(y_proba, y_holdout_true, epsilon=1.0e-15)\n",
    "print('Single RDF logloss = %.4f' % log_loss) \n",
    "\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini')\n",
    "clf.fit(X_train,y_train)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred = np.zeros(len(y_holdout_true))\n",
    "\n",
    "class_1_rows = np.where(y_proba >= 0.5)[0]\n",
    "\n",
    "y_pred[class_1_rows] = 1\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "MCC = matthews_corrcoef(y_holdout_true,y_pred)\n",
    "print('Single ET MCC = %.4f' % MCC)\n",
    "\n",
    "log_loss = logloss(y_proba, y_holdout_true, epsilon=1.0e-15)\n",
    "print('Single ET logloss = %.4f' % log_loss)     \n",
    "\n",
    "clf = GradientBoostingClassifier(learning_rate=0.05, \n",
    "                                 subsample=0.5, \n",
    "                                 max_depth=6, \n",
    "                                 n_estimators=50)\n",
    "clf.fit(X_train,y_train)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred = np.zeros(len(y_holdout_true))\n",
    "\n",
    "class_1_rows = np.where(y_proba >= 0.5)[0]\n",
    "\n",
    "y_pred[class_1_rows] = 1\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "MCC = matthews_corrcoef(y_holdout_true,y_pred)\n",
    "print('Single GBT MCC = %.4f' % MCC) \n",
    "\n",
    "log_loss = logloss(y_proba, y_holdout_true, epsilon=1.0e-15)\n",
    "print('Single GBT logloss = %.4f' % log_loss) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
